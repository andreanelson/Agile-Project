{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e25306",
   "metadata": {},
   "source": [
    "# Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47267638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b279e93",
   "metadata": {},
   "source": [
    "# Function used to scrape 'Online Shops' page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb3f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(url):\n",
    "    # Your URL cleaning logic here\n",
    "    cleaned_url = url.strip()  # For example, removing leading/trailing spaces\n",
    "    return cleaned_url\n",
    "\n",
    "def giveMyJson(csvFile):\n",
    "    # Data Cleaning\n",
    "    data = pd.read_csv(csvFile,encoding='utf-8')\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    data.dropna(subset=['Google Location', 'Name'], inplace=True)\n",
    "    data['Rating'].fillna(0, inplace=True)\n",
    "    desired_purposes = ['Pet supply store', 'Pet Shop', 'Pet food and animal feeds']\n",
    "    data = data[data['Purpose'].isin(desired_purposes)]\n",
    "    data['Address'].fillna('Address not listed', inplace=True)\n",
    "    data['Contact No.'].fillna('Contact not listed', inplace=True)\n",
    "    data = data.sort_values(by='Name').reset_index(drop=True)\n",
    "    \n",
    "    # Getting URL by webscraping\n",
    "    driver = webdriver.Chrome()\n",
    "    extracted_data = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        url = row['Google Location']\n",
    "        name = row['Name']\n",
    "        rating = row['Rating']\n",
    "        image = row['Image']\n",
    "        purpose = row['Purpose']\n",
    "        address = row['Address']\n",
    "        contact = row['Contact No.']\n",
    "    \n",
    "        try:   \n",
    "            driver.get(url)\n",
    "            script = 'return document.querySelector(\"a[data-tooltip=\\\\\"Open website\\\\\"]\").getAttribute(\"href\")'\n",
    "            title = driver.execute_script(script)\n",
    "            soup = BeautifulSoup(title, 'html.parser')\n",
    "            if soup:\n",
    "                url = soup.text\n",
    "            else:\n",
    "                url = \"URL not found\"\n",
    "        except Exception:\n",
    "            url = \"URL Not Listed\"\n",
    "    \n",
    "        # Append the extracted data to the list\n",
    "        extracted_data.append({'Name': name, 'URL': url,'Rating': rating,'Address': address,'Contact': contact,'Purpose': purpose,'Image Link': image})\n",
    "    \n",
    "    driver.quit()\n",
    "    data = pd.DataFrame(extracted_data)\n",
    "    \n",
    "    # Final cleaning of data and creating json file\n",
    "    data['URL'] = data['URL'].apply(clean_url)\n",
    "    data.drop_duplicates(subset=['URL'], inplace=True)\n",
    "    data = data[data['URL'] != 'URL Not Listed']\n",
    "    data.sort_values(by='Name', inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    json_data = []\n",
    "    for index, row in data.iterrows():\n",
    "        store_info = {\n",
    "            \"name\": row[\"Name\"],\n",
    "            \"url\": row[\"URL\"],\n",
    "            \"rating\": row[\"Rating\"],\n",
    "            \"address\": row[\"Address\"],\n",
    "            \"contact\": row[\"Contact\"],\n",
    "            \"purpose\": row[\"Purpose\"],\n",
    "            \"image_link\": row[\"Image Link\"]\n",
    "        }\n",
    "        json_data.append(store_info)\n",
    "\n",
    "    # Define the output JSON file name\n",
    "    json_filename = \"pet_stores.json\"\n",
    "\n",
    "    # Write the JSON data to the file\n",
    "    with open(json_filename, 'w') as json_file:\n",
    "        json.dump(json_data, json_file, indent=4)\n",
    "        \n",
    "# This function basically deals with what you want to have in your json file,provided csv file exported from instant data scraper\n",
    "giveMyJson('google.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214ad92",
   "metadata": {},
   "source": [
    "The function above takes a csv as an argument, which is basically the data scraped using the 'Instant Data Scraper' chrome extension. The data is then cleaned and the website url is scraped from the google search result which corresponds to that online store. After extracting the website url, the data is cleaned again and a json file containing that data is given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d588aa0",
   "metadata": {},
   "source": [
    "# Function used to scrape 'Top Selling Products' page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24062d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_google_shopping(query):\n",
    "    # Initialize Chrome WebDriver (you need to download and specify the path to your ChromeDriver executable)\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Navigate to Google Shopping with the specified query\n",
    "    search_query = f'https://www.google.com/search?q={query}&tbm=shop'\n",
    "    driver.get(search_query)\n",
    "\n",
    "    # Wait for the page to load (you may need to adjust the waiting time)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Scroll down the page to load more results (you can adjust the number of scrolls)\n",
    "    for _ in range(5):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Get the page source after scrolling\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Find and extract the product elements (you may need to inspect the HTML structure)\n",
    "    product_elements = soup.find_all('div', class_='sh-dgr__gr-auto sh-dgr__grid-result')\n",
    "\n",
    "    # Initialize a list to store product information\n",
    "    products = []\n",
    "\n",
    "    # Iterate through product elements\n",
    "    for product_element in product_elements:\n",
    "        product_name = product_element.find('h3', class_='tAxDx').text.strip()\n",
    "        product_price = product_element.find('span', class_='a8Pemb').text.strip()\n",
    "\n",
    "        # Extract the product URL\n",
    "        product_url_element = product_element.find('a', class_='shntl', href=True)\n",
    "        if product_url_element:\n",
    "            encoded_url = product_url_element['href']\n",
    "            # Decode the URL and construct the full URL\n",
    "            decoded_url = urllib.parse.unquote(encoded_url)\n",
    "            full_url = \"https://www.google.com\" + decoded_url\n",
    "            product_url = full_url\n",
    "        else:\n",
    "            product_url = None\n",
    "\n",
    "        # Extracting image URL\n",
    "        product_image_element = product_element.find('div', class_='ArOc1c')\n",
    "        product_image_element = product_image_element.find('img', src=True)\n",
    "        if product_image_element:\n",
    "            product_image = product_image_element['src']\n",
    "        else:\n",
    "            product_image = None\n",
    "            \n",
    "        # Extracting ratings\n",
    "        product_ratings_element = product_element.find('span', class_='Rsc7Yb')\n",
    "        if product_ratings_element:\n",
    "            product_ratings = product_ratings_element.text.strip()\n",
    "        else:\n",
    "            product_ratings = None\n",
    "\n",
    "        products.append({\n",
    "            'name': product_name,\n",
    "            'price': product_price,\n",
    "            'url': product_url,\n",
    "            'image': product_image,\n",
    "            'ratings': product_ratings\n",
    "        })\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    # Convert the product data to JSON\n",
    "    json_data = json.dumps(products, indent=4)\n",
    "\n",
    "    # Save the JSON data to a file\n",
    "    with open('products.json', 'w', encoding='utf-8') as json_file:\n",
    "        json_file.write(json_data)\n",
    "\n",
    "# application:\n",
    "query = 'top selling products for cats and dogs in Singapore'\n",
    "scrape_google_shopping(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45228cab",
   "metadata": {},
   "source": [
    "The function above takes a google query as an input argument, which then scrapes the results of that query. The out is a json file consisting of the product information, which is scraped by identifying the correct html tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866afaf",
   "metadata": {},
   "source": [
    "# Function used to scrape 'Pet Care Info' page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab035be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Replace '\\u2019' with a regular apostrophe\n",
    "    return text.replace('\\u2019', \"'\").strip()\n",
    "\n",
    "def scrape_website(url, selectors):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for 4xx and 5xx HTTP status codes\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        data = []\n",
    "\n",
    "        tip_elements = soup.select(selectors[\"tip\"])\n",
    "        desc_elements = []\n",
    "\n",
    "        # Check if a separate selector is provided for descriptions\n",
    "        if \"desc\" in selectors:\n",
    "            desc_elements = soup.select(selectors[\"desc\"])\n",
    "\n",
    "        for tip_element in tip_elements:\n",
    "            tip_text = re.sub(r'^\\d+\\.\\s*', '', clean_text(tip_element.text))  # Remove numbering\n",
    "            description = \"\"\n",
    "\n",
    "            # Find the description associated with this tip element\n",
    "            if desc_elements:\n",
    "                # Look for the description among siblings of the tip element\n",
    "                for sibling in tip_element.find_next_siblings():\n",
    "                    if sibling in desc_elements:\n",
    "                        description = clean_text(sibling.text)\n",
    "                        break\n",
    "\n",
    "            # Check if the grooming tip is not empty before adding it\n",
    "            if tip_text.strip():\n",
    "                data.append({\n",
    "                    \"website_url\": url,\n",
    "                    \"grooming_tip\": tip_text,\n",
    "                    \"description\": description,\n",
    "                })\n",
    "\n",
    "        return data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"error\": str(e)}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def main():\n",
    "    # Define rules for each website, including selectors\n",
    "    website_rules = {\n",
    "        \"hastingsvet.com\": {\n",
    "            \"url\": \"https://hastingsvet.com/six-helpful-grooming-tips-for-your-dog-or-cat/\",\n",
    "            \"selectors\": {\n",
    "                \"tip\": \"div.elementor-widget-container h2.wp-block-heading\",\n",
    "                \"desc\": \"div.elementor-widget-container p\",\n",
    "            },\n",
    "        },\n",
    "        \"briopets.com\": {\n",
    "            \"url\": \"https://briopets.com/blogs/briopets-official-blog/top-9-pet-grooming-tips-for-dog-and-cat-owners\",\n",
    "            \"selectors\": {\n",
    "                \"tip\": \"div.rte h3\",\n",
    "                \"desc\": \"div.rte p\",\n",
    "            },\n",
    "        },\n",
    "        \"revivalanimal.com\": {\n",
    "            \"url\": \"https://www.revivalanimal.com/learning-center/pet-grooming-tips-tricks\",\n",
    "            \"selectors\": {\n",
    "                \"tip\": \"h3 + ul li\",\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for domain, rule in website_rules.items():\n",
    "        print(f\"Scraping from {domain}\")\n",
    "        data = scrape_website(rule[\"url\"], rule[\"selectors\"])\n",
    "        results.extend(data)\n",
    "\n",
    "    # Remove entries with empty grooming tips\n",
    "    results = [entry for entry in results if entry[\"grooming_tip\"].strip()]\n",
    "\n",
    "    if results:\n",
    "        json_data = json.dumps(results, indent=4)\n",
    "#         print(json_data)\n",
    "#         You can write the JSON data to a file if needed\n",
    "        with open('grooming_tips.json', 'w', encoding='utf-8') as file:\n",
    "            file.write(json_data)\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c437967",
   "metadata": {},
   "source": [
    "The function above is robust such that we just needed to add website urls which provided pet care information such as the ones listed under website rules. We will also need to pass the relavant tags which will be needed for scraping. The function 'scrape_website' is the main function used for scraping data. The output again is a json file with our data stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1f085",
   "metadata": {},
   "source": [
    "# Function used to scrape data for 'Shops' page\n",
    "###### Applicable for all the pages with maps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51121c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converted the xslx file to csv \n",
    "input_excel = \"vet.xlsx\"\n",
    "output_csv = \"vet.csv\"\n",
    "\n",
    "# Read Excel file\n",
    "df = pd.read_excel(input_excel)\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Initialize empty lists for each column\n",
    "s_n_list = []\n",
    "type_list = []\n",
    "name_list = []\n",
    "address_list = []\n",
    "tel_office_list = []\n",
    "\n",
    "# Open the CSV file for reading\n",
    "with open('vet.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "\n",
    "    # Skip the header row\n",
    "    next(csvreader)\n",
    "\n",
    "    # Iterate through each row and extract data for each column\n",
    "    for row in csvreader:\n",
    "        if len(row) >= 5:  # Ensure that the row has at least 5 columns\n",
    "            s_n = row[0].strip()\n",
    "            type = row[1].strip()\n",
    "            name = row[2].strip()\n",
    "            address = row[3].strip()\n",
    "            tel_office = row[4].strip()\n",
    "\n",
    "            # Append data to respective lists\n",
    "            s_n_list.append(s_n)\n",
    "            type_list.append(type)\n",
    "            name_list.append(name)\n",
    "            address_list.append(address)\n",
    "            tel_office_list.append(tel_office)\n",
    "\n",
    "with open('vet.json', 'r', encoding='utf-8') as jsonfile:\n",
    "    data = json.load(jsonfile)\n",
    "\n",
    "# Initialize a new list to store the updated data\n",
    "new_data_list = []\n",
    "\n",
    "# Iterate through each entry in the existing data\n",
    "for entry in data:\n",
    "    name_address_pair = entry.get('  NAME    ;ADDRESS', '').split(';')\n",
    "    if len(name_address_pair) == 2:\n",
    "        name, address = name_address_pair\n",
    "        name = name.strip()\n",
    "        address = address.strip()\n",
    "        new_data_list.append({\"NAME\": name, \"ADDRESS\": address})\n",
    "\n",
    "# Specify the name of the new JSON output file\n",
    "new_output_json_file = 'vet_new.json'\n",
    "\n",
    "# Load the JSON data\n",
    "input_json = \"vet_new.json\"  \n",
    "output_json = \"output.json\"  \n",
    "\n",
    "with open(input_json, \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Process the data to split the \"name\" and \"address\" based on the delimiter \";\"\n",
    "processed_data = []\n",
    "for entry in data:\n",
    "    name_address = entry[\"  NAME    ;ADDRESS\"].strip()\n",
    "    null_values = entry.get(\"null\", [])\n",
    "\n",
    "    # Split the name and address based on the delimiter \";\"\n",
    "    parts = name_address.split(\";\")\n",
    "    name = parts[0].strip()\n",
    "    address = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "\n",
    "    # Merge the \"null\" values with the address\n",
    "    if null_values:\n",
    "        address += \" \" + \" \".join(null_values)\n",
    "\n",
    "    processed_data.append({\"name\": name, \"address\": address})\n",
    "\n",
    "# Load the JSON file\n",
    "input_json = \"vet_new.json\"\n",
    "output_json = \"vet_geocode.json\"\n",
    "api_key = \"AIzaSyCUax1ygLocmDPvzRYPbfr1zVzo6M6YNKM\"  # Replace with your actual API key\n",
    "\n",
    "# Load the JSON data\n",
    "with open(input_json, \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Define the base URL for Geocoding API\n",
    "geocode_base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "\n",
    "# Update each entry with latitude and longitude\n",
    "for entry in data:\n",
    "    address = entry[\"address\"]\n",
    "    params = {\n",
    "        \"address\": address,\n",
    "        \"key\": api_key\n",
    "    }\n",
    "    response = requests.get(geocode_base_url, params=params)\n",
    "    result = response.json()\n",
    "    \n",
    "    # Check if there's a valid result\n",
    "    if result[\"status\"] == \"OK\":\n",
    "        location = result[\"results\"][0][\"geometry\"][\"location\"]\n",
    "        entry[\"latitude\"] = location[\"lat\"]\n",
    "        entry[\"longitude\"] = location[\"lng\"]\n",
    "    else:\n",
    "        print(f\"Geocoding failed for address: {address}\")\n",
    "\n",
    "# Save the data with latitude and longitude to a new JSON file\n",
    "with open(output_json, \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=4)\n",
    "\n",
    "print(f\"Geocoding results have been added to '{output_json}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
